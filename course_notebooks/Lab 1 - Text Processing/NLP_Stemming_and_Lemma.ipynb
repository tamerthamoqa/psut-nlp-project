{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDO8Bp0cgQMh",
    "outputId": "369f4a96-a3e0-4e95-de85-4dbe800332aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skBbOHISvk3Q"
   },
   "source": [
    "### **Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l2zop3lxiNVJ",
    "outputId": "3aa505ec-2e35-46e5-c0f1-1f7f375fa532"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "#All Languages that SnowballStemmer can support!!\n",
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8i19OV3ViXDK",
    "outputId": "0282b166-7b17-404f-e3b8-fef104403754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter=> have\n",
      "Snowball=> having\n",
      "Lancaster=> hav\n",
      "\n",
      "Porter=> gener\n",
      "Snowball=> generous\n",
      "Lancaster=> gen\n",
      "\n",
      "Porter=> run\n",
      "Snowball=> run\n",
      "Lancaster=> run\n",
      "\n",
      "Porter=> embarrass\n",
      "Snowball=> embarrass\n",
      "Lancaster=> embarrass\n",
      "\n",
      "Porter=> friendship\n",
      "Snowball=> friendship\n",
      "Lancaster=> friend\n",
      "\n",
      "Porter=> cach\n",
      "Snowball=> cach\n",
      "Lancaster=> cach\n"
     ]
    }
   ],
   "source": [
    "# Difference between PorterStemmer and SnowballStemmer\n",
    "\n",
    "Pstemmer = PorterStemmer()\n",
    "SNstemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "LNstemmer =LancasterStemmer()\n",
    "#e.g.1\n",
    "print('Porter=> ' + Pstemmer.stem(\"having\"))\n",
    "print('Snowball=> ' + SNstemmer.stem(\"having\"))\n",
    "print('Lancaster=> ' + LNstemmer.stem(\"having\"))\n",
    "print()\n",
    "#e.g.2\n",
    "print('Porter=> ' + Pstemmer.stem(\"generously\"))\n",
    "print('Snowball=> ' + SNstemmer.stem(\"generously\"))\n",
    "print('Lancaster=> ' + LNstemmer.stem(\"generously\"))\n",
    "print()\n",
    "#e.g.3\n",
    "print('Porter=> ' + Pstemmer.stem(\"running\"))\n",
    "print('Snowball=> ' + SNstemmer.stem(\"running\"))\n",
    "print('Lancaster=> ' + LNstemmer.stem(\"running\"))\n",
    "print()\n",
    "#e.g.4\n",
    "print('Porter=> ' + Pstemmer.stem(\"embarrassment\"))\n",
    "print('Snowball=> ' + SNstemmer.stem(\"embarrassment\"))\n",
    "print('Lancaster=> ' + LNstemmer.stem(\"embarrassment\"))\n",
    "print()\n",
    "#e.g.4\n",
    "print('Porter=> ' + Pstemmer.stem(\"friendships\"))\n",
    "print('Snowball=> ' + SNstemmer.stem(\"friendships\"))\n",
    "print('Lancaster=> ' + LNstemmer.stem(\"friendships\"))\n",
    "print()\n",
    "#e.g.4\n",
    "print('Porter=> ' + Pstemmer.stem(\"cached\"))\n",
    "print('Snowball=> ' + SNstemmer.stem(\"cached\"))\n",
    "print('Lancaster=> ' + LNstemmer.stem(\"cached\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "omwG3p29hsEB",
    "outputId": "369d9a04-f80d-40ea-ec17-59c732ad5f4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied', \n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eBoKjXk5kbhZ",
    "outputId": "c99baa73-4710-476a-f438-885bbe6ce520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "#Porter Stemmer\n",
    "singles = [Pstemmer.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CHO8BrjkeCv",
    "outputId": "a5aa293b-8746-4c57-8db6-1e79e5752353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "#SnowballStemmer\n",
    "singles = [SNstemmer.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z7OJb3gCuCgi",
    "outputId": "c3eabcf9-fa05-415c-d74d-3fcb85b064f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli die mul deny died agree own humbl siz meet stat siez item sens tradit ref colon plot\n"
     ]
    }
   ],
   "source": [
    "#LancasterStemmer\n",
    "singles = [LNstemmer.stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gn9QGm2tkYYo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VPsBTRLuiq9C",
    "outputId": "bc5e1c2f-0aa6-4263-b6be-cc17704318ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".قضي في خليل ستة شهر ثم بدا اهل بلد يتوزع منهم من ارد لحاق باقرباء له في طولكرم او ابلس او جني و منهم من سلل عائد الي جليل و منهم من ذهب الي اردن . حرب خيف كثير لكن علم علم حرب ان كلم قد لا عني مدلول بشكل كامل . نعم زرت عما زرت بعد خمس سني من غياب و ذلك عام 1973 . لجم جميع لا احد ستطيع فسير ما حصل يتمتم جيوش عربي تدخل اجل تدخل تاكيد هل يتر عرب سطين ضيع . رحل الي بيرو تظن نهاي حرب بداي ستقرر يخو حدس حرب هنا غير تلك في سطين لكن حرب لتي لا تعب بك بمن تكو .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.arlstem import ARLSTem\n",
    "ARstemmer = ARLSTem()\n",
    "ARstemmer.stem('يعمل')\n",
    "\n",
    "# Arabic Language\n",
    "arab_sent=\".قضينا في الخليل ستة أشهر ثم بدأ أهل البلد يتوزعون منهم من أراد اللحاق بأقرباء له في طولكرم أو نابلس أو جنين و منهم من تسلل عائدا الى الجليل و منهم من ذهب الى الاردن.  الحرب تخيف كثيراً لكنها تعلم تعلمك الحرب أن الكلمات قد لا تعني مدلولاتها بشكل كامل. نعم زرت عمان زرتها بعد خمس سنين من الغياب و ذلك عام 1973. ويلجم الجميع لا أحد يستطيع تفسير ما حصل يتمتمون الجيوش العربية ستتدخل أجل ستتدخل بالتأكيد هل يترك العرب فلسطين تضيع. ترحل إلى بيروت تظنها نهاية الحرب وبداية الاستقرار يخونها حدسها الحرب هنا غير تلك في فلسطين لكنها الحرب التي لا تعبأ بك وبمن تكون.\" \n",
    "stmt= word_tokenize(arab_sent)\n",
    "singles = [ARstemmer.stem(s) for s in stmt]\n",
    "print(' '.join(singles))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iOIxay5uKzR"
   },
   "source": [
    "### **Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RV9XtiUcuNug",
    "outputId": "697aa3fe-96b3-4c15-cf70-efd859ec54e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress\n",
      "fly\n",
      "die\n",
      "mules\n",
      "deny\n",
      "die\n",
      "agree\n",
      "own\n",
      "humble\n",
      "size\n",
      "meet\n",
      "state\n",
      "siezing\n",
      "itemization\n",
      "sensational\n",
      "traditional\n",
      "reference\n",
      "colonizer\n",
      "plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "WNL=WordNetLemmatizer()\n",
    "for w in plurals:\n",
    "  print(WNL.lemmatize(w, pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Va8thfELjktp"
   },
   "source": [
    "### **StopWords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00sdWUbsjnYQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7bpF7Z3gdE6",
    "outputId": "d588c261-08be-4633-9b44-2ec1793728f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"won't\", 'o', 'weren', 'aren', \"doesn't\", 'from', 've', 'why', 'who', 'for', \"you're\", 'they', \"needn't\", 'as', 'our', 'own', 'such', 't', 'be', 'yourselves', 'his', 'down', 'that', 'doesn', 'these', \"haven't\", 'hers', 'them', 'which', 'the', 'further', 'do', 'against', 'ain', 'myself', 'of', 'no', 'below', \"couldn't\", 'to', 'shouldn', 'but', \"wasn't\", 'again', 'by', 'when', \"aren't\", 'above', 'was', \"don't\", 'i', 'same', 'has', 'up', 'ourselves', 'had', 'only', 'if', 'were', 'don', 'their', 'under', 'needn', 'out', 'itself', \"hasn't\", 'are', 'its', \"didn't\", \"mustn't\", 'and', 're', \"she's\", 's', 'is', 'am', 'me', 'before', \"wouldn't\", 'she', \"weren't\", 'wasn', \"isn't\", 'wouldn', 'about', 'it', 'should', 'himself', 'theirs', 'each', 'where', 'while', 'nor', 'very', 'this', 'until', 'ours', 'you', 'during', \"hadn't\", 'any', 'not', 'themselves', 'won', 'can', 'most', 'ma', \"it's\", 'or', 'yours', 'what', 'didn', 'will', 'both', 'd', 'some', 'being', 'in', 'him', \"you'll\", 'at', 'herself', 'isn', 'too', 'having', \"mightn't\", 'on', 'doing', 'haven', 'we', 'm', 'shan', 'into', \"you've\", 'whom', 'there', 'few', 'than', 'now', 'yourself', \"shouldn't\", 'how', 'off', 'a', 'other', 'y', 'have', 'after', 'did', 'all', \"you'd\", 'then', 'couldn', 'with', 'mightn', 'hadn', 'my', 'just', \"shan't\", 'more', 'mustn', \"that'll\", 'your', 'been', 'once', 'll', 'does', \"should've\", 'he', 'over', 'those', 'between', 'so', 'hasn', 'through', 'an', 'her', 'here', 'because'}\n"
     ]
    }
   ],
   "source": [
    "#Stop Words, you can change it arabic\n",
    "stop_words=set(stopwords.words('english'),)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E-rzpKYjjysw",
    "outputId": "7c451eed-b226-4adc-b267-864bd9764025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ل', 'كانون', 'ست', 'أفٍّ', 'هبّ', 'أيلول', 'اللتان', 'آض', 'ثمانين', 'لوما', 'ة', 'تعلَّم', 'إنا', 'نَّ', 'تي', 'إياها', 'أنّى', 'بمن', 'فيفري', 'كلا', 'انقلب', 'قلما', 'منها', 'والذين', 'وإذ', 'س', 'هَذَيْنِ', 'أوّهْ', 'أل', 'كأي', 'هؤلاء', 'كم', 'كلتا', 'أمام', 'نحو', 'جويلية', 'حتى', 'ذلكم', 'كأيّن', 'ثلاثون', 'شَتَّانَ', 'فضلا', 'فيم', 'متى', 'ت', 'زود', 'ط', 'كأين', 'ف', 'أفعل به', 'سحقا', 'إن', 'ثلاثاء', 'ليس', 'ذان', 'ورد', 'أجمع', 'لسن', 'سرعان', 'وإذا', 'عيانا', 'إمّا', 'ترك', 'كان', 'أمامكَ', 'حبذا', 'إزاء', 'راء', 'تبدّل', 'ثمة', 'ذلكن', 'ء', 'رابع', 'تسعين', 'وا', 'تسعون', 'ثلاثين', 'ذوا', 'أنتن', 'إنه', 'كذا', 'خمسمئة', 'طَق', 'ثمانية', 'شبه', 'ماذا', 'حجا', 'هي', 'وإن', 'وهو', 'أخو', 'لا سيما', 'أنت', 'ذواتي', 'كيف', 'مافتئ', 'هَاتَيْنِ', 'بين', 'بك', 'كي', 'ق', 'أربعة', 'حزيران', 'ذ', 'تارة', 'كليكما', 'اخلولق', 'خاصة', 'أمّا', 'دينار', 'يفعلون', 'يا', 'عل', 'رزق', 'ثمانون', 'اللائي', 'حيث', 'بغتة', 'لكم', 'ذِي', 'هاء', 'تجاه', 'صاد', 'تلكما', 'أولئك', 'آها', 'سابع', 'خبَّر', 'عن', 'أربعاء', 'فلان', 'اللواتي', 'نون', 'طالما', 'رُبَّ', 'خمسين', 'نحن', 'كأيّ', 'قام', 'ى', 'لما', 'تسع', 'ذلكما', 'زاي', 'ليستا', 'أُفٍّ', 'وراءَك', 'ألفى', 'حمدا', 'شباط', 'بهن', 'إيه', 'لعلَّ', 'بهم', 'هاك', 'اللذين', 'عجبا', 'سوى', 'و', 'واحد', 'أيا', 'على', 'فيه', 'حدَث', 'شيكل', 'شمال', 'مع', 'حبيب', 'الألاء', 'آذار', 'إليكنّ', 'أمس', 'لسنا', 'أسكن', 'أنا', 'مساء', 'يوان', 'ذانِ', 'هيت', 'مثل', 'أخذ', 'عشرين', 'دولار', 'ج', 'تانِك', 'بكما', 'ليسوا', 'ز', 'هَجْ', 'لاسيما', 'ثمَّ', 'قرش', 'هاتين', 'لبيك', 'هكذا', 'جيم', 'واهاً', 'لن', 'إياهن', 'بكن', 'اتخذ', 'إحدى', 'ما انفك', 'غير', 'مرّة', 'ثلاثمائة', 'لم', 'ن', 'خامس', 'ثلاثة', 'نيف', 'عَدَسْ', 'تانِ', 'بها', 'عليه', 'هل', 'بعد', 'شتانَ', 'آهٍ', 'حَذارِ', 'دون', 'ب', 'كليهما', 'بيد', 'مادام', 'ليست', 'سبحان', 'ارتدّ', 'جنيه', 'أن', 'أنتِ', 'مئة', 'آنفا', 'جمعة', 'هَاتانِ', 'ذاك', 'تعسا', 'صبرا', 'هذين', 'حاي', 'كلَّا', 'إذاً', 'بضع', 'أصلا', 'ثلاث', 'ألف', 'تِي', 'هما', 'ثمّة', 'ولو', 'ض', 'لمّا', 'بما', 'أربعمائة', 'جير', 'ممن', 'هن', 'تاء', 'شرع', 'فإن', 'أقبل', 'هنا', 'ديسمبر', 'يمين', 'أوه', 'إذما', 'صدقا', 'أيها', 'الذي', 'تفعلين', 'تين', 'في', 'رويدك', 'أعلم', 'تسعة', 'خميس', 'أنًّ', 'رأى', 'حمٌ', 'أطعم', 'ميم', 'طفق', 'تخذ', 'خلا', 'اللتين', 'التي', 'حادي', 'عدَّ', 'تسعمائة', 'يوليو', 'باء', 'ثمانمئة', 'هَؤلاء', 'غدا', 'ذي', 'راح', 'آناء', 'درهم', 'تشرين', 'إلّا', 'جانفي', 'إيهٍ', 'هلا', 'لكنَّ', 'قد', 'ذيت', 'ا', 'نيسان', 'خ', 'حيَّ', 'بس', 'أوت', 'إليكَ', 'ؤ', 'لهما', 'هَذانِ', 'ستون', 'طاء', 'جلل', 'مكانكم', 'صهٍ', 'كأنما', 'كرب', 'سبتمبر', 'علًّ', 'خمس', 'ذال', 'فمن', 'ثان', 'ذينك', 'لك', 'حيثما', 'لكما', 'هَذِه', 'مازال', 'أو', 'ولا', 'شين', 'عما', 'صبر', 'أغسطس', 'عين', 'أول', 'اثنان', 'رجع', 'كسا', 'سبع', 'ّأيّان', 'فيها', 'أنبأ', 'أوشك', 'مئتان', 'أجل', 'صهْ', 'مه', 'إذن', 'اللتيا', 'غين', 'خمسة', 'بعدا', 'طرا', 'ذواتا', 'سنتيم', 'حين', 'تِه', 'دونك', 'ئ', 'فرادى', 'بهما', 'ءَ', 'كيفما', 'مهما', 'كيت', 'ظاء', 'إياهم', 'كل', 'أربع', 'لئن', 'كأن', 'لي', 'نَخْ', 'تسعمئة', 'آه', 'هو', 'اثني', 'زعم', 'قاطبة', 'بَسْ', 'بكم', 'به', 'يناير', 'ها', 'نوفمبر', 'ذات', 'ما أفعله', 'نفس', 'اربعون', 'حقا', 'أين', 'سرا', 'ثلاثمئة', 'عاد', 'ليرة', 'أولالك', 'سبعون', 'أيضا', 'كاد', 'ألا', 'خاء', 'ثاني', 'خلف', 'لكي', 'علق', 'واو', 'مايو', 'مكانكما', 'ستمئة', 'تاسع', 'إياه', 'عشرون', 'د', 'بلى', 'ث', 'بل', 'هللة', 'إياك', 'عامة', 'هَاتِي', 'فو', 'لهم', 'ته', 'أم', 'أى', 'لكن', 'لستم', 'ه', 'ثاء', 'فاء', 'ذهب', 'كأنّ', 'إليك', 'إليكما', 'م', 'بخٍ', 'جوان', 'أخٌ', 'مكانكنّ', 'أصبح', 'نعم', 'حسب', 'أبدا', 'خمسمائة', 'إياكن', 'همزة', 'أمسى', 'أربعمئة', 'تفعلون', 'غ', 'إياكما', 'سمعا', 'كى', 'أيّان', 'بئس', 'أمد', 'منذ', 'عشرة', 'أنتم', 'آهاً', 'ذا', 'لدن', 'يفعلان', 'ذِه', 'معاذ', 'لو', 'آب', 'إياي', 'الآن', 'ياء', 'أخبر', 'أف', 'أولاء', 'لهن', 'كذلك', 'غادر', 'بخ', 'لها', 'ذَيْنِ', 'ح', 'جعل', 'آي', 'إلى', 'ر', 'حاشا', 'هاتي', 'مكانَك', 'وهب', 'وَيْ', 'سوف', 'قاف', 'هم', 'كما', 'عند', 'ثماني', 'كلّما', 'هاهنا', 'فلس', 'هذان', 'هاكَ', 'منه', 'ص', 'نبَّا', 'ومن', 'شتان', 'عاشر', 'فإذا', 'صار', 'أقل', 'هَذِي', 'آهِ', 'بي', 'ماي', 'الذين', 'كلما', 'حاء', 'لولا', 'أ', 'أيار', 'إما', 'أبٌ', 'كاف', 'إذ', 'ما', 'أنى', 'هاتان', 'هذه', 'تَيْنِ', 'ثامن', 'ستة', 'أهلا', 'إنما', 'مما', 'كلاهما', 'كن', 'هذي', 'عشر', 'بنا', 'إذا', 'أينما', 'عدا', 'وما', 'مارس', 'سين', 'طاق', 'خلافا', 'ذو', 'صراحة', 'ساء', 'ثالث', 'إى', 'خال', 'سقى', 'قبل', 'ليسا', 'سادس', 'له', 'ستمائة', 'مذ', 'هنالك', 'تلك', 'هيهات', 'ما برح', 'هلم', 'ظ', 'إياهما', 'ابتدأ', 'كثيرا', 'اللذان', 'أنتما', 'ذلك', 'ش', 'دال', 'إليكن', 'دواليك', 'ذين', 'ثمّ', 'نا', 'ثمنمئة', 'غالبا', 'هَذا', 'ظنَّ', 'أنشأ', 'ليت', 'أكتوبر', 'اربعين', 'إنَّ', 'لستما', 'ثم', 'حرى', 'خمسون', 'إلَيْكَ', 'بَلْهَ', 'ريث', 'إياكم', 'ظلّ', 'هذا', 'هاته', 'ع', 'ذانك', 'قطّ', 'لعل', 'أكثر', 'فيما', 'سبت', 'يورو', 'بماذا', 'عسى', 'حمو', 'اثنا', 'سبعة', 'ريال', 'عليك', 'لكيلا', 'أيّ', 'أبو', 'بات', 'من', 'أرى', 'غداة', 'هَيْهات', 'أمامك', 'هيّا', 'سبعمئة', 'بؤسا', 'ك', 'ي', 'لدى', 'وُشْكَانَ', 'اثنين', 'ستين', 'اللاتي', 'كِخ', 'انبرى', 'فوق', 'أضحى', 'أما', 'أي', 'ضاد', 'وجد', 'فلا', 'لنا', 'درى', 'تفعلان', 'ين', 'تموز', 'ثمان', 'لستن', 'لام', 'هيا', 'ذه', 'سبعمائة', 'فبراير', 'ولكن', 'تينك', 'إليكم', 'آمينَ', 'بسّ', 'تلقاء', 'لات', 'أفريل', 'أحد', 'لكنما', 'والذي', 'إي', 'تلكم', 'ضحوة', 'إلا', 'لا', 'أبريل', 'هناك', 'يونيو', 'عوض', 'سبعين', 'استحال', 'هلّا', 'أعطى', 'تحت', 'لست', 'لعمر', 'آ', 'مليم', 'هَاتِه', 'بعض', 'الألى', 'بطآن', 'إيانا', 'تحوّل', 'حار', 'جميع', 'صباح', 'مائة', 'علم'}\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words('arabic'),)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6RNLtfKjqcR"
   },
   "source": [
    "### **Text Processing Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "my89Fl51gVqu"
   },
   "outputs": [],
   "source": [
    "# This function makes text processing (Tokenization, StopWords and Stemming)\n",
    "def TextProcessing(sentence):\n",
    "  sentence=sentence.lower()\n",
    "  ps=PorterStemmer()\n",
    "  tokenizer= RegexpTokenizer(r\"\\w+\")\n",
    "  terms=tokenizer.tokenize(sentence)\n",
    "  stmt=[]\n",
    "  for term in terms: \n",
    "    if term not in stop_words:\n",
    "      stmt.append(ps.stem(term))\n",
    "  return \" \".join(stmt)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "skBbOHISvk3Q",
    "1iOIxay5uKzR",
    "Va8thfELjktp",
    "t6RNLtfKjqcR"
   ],
   "name": "NLP-Stemming and Lemma.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
